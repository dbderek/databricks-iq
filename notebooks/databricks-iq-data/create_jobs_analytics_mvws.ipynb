{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89b8ff52-8421-4a69-992e-0e7af0ee5031",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Jobs Monitoring\n",
    "#### Based on Databricks documentation for tracking jobs spend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "289ae036-78ce-4e05-92fa-32ca11a61dc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "-- Most expensive jobs in last 30 days\n",
    "CREATE OR REPLACE MATERIALIZED VIEW most_expensive_jobs AS\n",
    "with list_cost_per_job as (\n",
    "  SELECT\n",
    "    usage_data.workspace_id,\n",
    "    usage_data.usage_metadata.job_id,\n",
    "    COUNT(DISTINCT usage_data.usage_metadata.job_run_id) as runs,\n",
    "    SUM(usage_data.usage_quantity * pricing_data.pricing.default) as list_cost,\n",
    "    SUM(usage_data.usage_quantity * pricing_data.pricing.effective_list.default) as effective_cost,\n",
    "    first(identity_metadata.run_as, true) as run_as,\n",
    "    first(usage_data.custom_tags, true) as custom_tags,\n",
    "    MAX(usage_data.usage_end_time) as last_seen_date\n",
    "  FROM system.billing.usage usage_data\n",
    "  INNER JOIN system.billing.list_prices pricing_data on\n",
    "    usage_data.cloud = pricing_data.cloud and\n",
    "    usage_data.sku_name = pricing_data.sku_name and\n",
    "    usage_data.usage_start_time >= pricing_data.price_start_time and\n",
    "    (usage_data.usage_end_time <= pricing_data.price_end_time or pricing_data.price_end_time is null)\n",
    "  WHERE\n",
    "    usage_data.billing_origin_product = \"JOBS\"\n",
    "    AND usage_data.usage_date >= CURRENT_DATE() - INTERVAL 30 DAY\n",
    "  GROUP BY ALL\n",
    "),\n",
    "most_recent_jobs as (\n",
    "  SELECT\n",
    "    *,\n",
    "    ROW_NUMBER() OVER(PARTITION BY workspace_id, job_id ORDER BY change_time DESC) as rn\n",
    "  FROM\n",
    "    system.lakeflow.jobs QUALIFY rn=1\n",
    ")\n",
    "SELECT\n",
    "    job_metadata.name,\n",
    "    spending_by_job.job_id,\n",
    "    spending_by_job.workspace_id,\n",
    "    spending_by_job.runs,\n",
    "    spending_by_job.run_as,\n",
    "    SUM(list_cost) as list_cost,\n",
    "    SUM(effective_cost) as effective_cost,\n",
    "    spending_by_job.custom_tags,\n",
    "    spending_by_job.last_seen_date\n",
    "FROM list_cost_per_job spending_by_job\n",
    "  LEFT JOIN most_recent_jobs job_metadata USING (workspace_id, job_id)\n",
    "GROUP BY ALL\n",
    "ORDER BY list_cost DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7438e9c3-d76e-4325-ba84-52d373e494eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "-- Most expensive job runs in last 30 days\n",
    "CREATE OR REPLACE MATERIALIZED VIEW most_expensive_job_runs AS\n",
    "with list_cost_per_job_run as (\n",
    "  SELECT\n",
    "    usage_data.workspace_id,\n",
    "    usage_data.usage_metadata.job_id,\n",
    "    usage_data.usage_metadata.job_run_id as run_id,\n",
    "    SUM(usage_data.usage_quantity * pricing_data.pricing.default) as list_cost,\n",
    "    SUM(usage_data.usage_quantity * pricing_data.pricing.effective_list.default) as effective_cost,\n",
    "    first(identity_metadata.run_as, true) as run_as,\n",
    "    first(usage_data.custom_tags, true) as custom_tags,\n",
    "    MAX(usage_data.usage_end_time) as last_seen_date\n",
    "  FROM system.billing.usage usage_data\n",
    "  INNER JOIN system.billing.list_prices pricing_data on\n",
    "    usage_data.cloud = pricing_data.cloud and\n",
    "    usage_data.sku_name = pricing_data.sku_name and\n",
    "    usage_data.usage_start_time >= pricing_data.price_start_time and\n",
    "    (usage_data.usage_end_time <= pricing_data.price_end_time or pricing_data.price_end_time is null)\n",
    "  WHERE\n",
    "    usage_data.billing_origin_product = 'JOBS'\n",
    "    AND usage_data.usage_date >= CURRENT_DATE() - INTERVAL 30 DAY\n",
    "  GROUP BY ALL\n",
    "  ),\n",
    "  most_recent_jobs as (\n",
    "    SELECT\n",
    "      *,\n",
    "      ROW_NUMBER() OVER(PARTITION BY workspace_id, job_id ORDER BY change_time DESC) as rn\n",
    "    FROM\n",
    "      system.lakeflow.jobs QUALIFY rn=1\n",
    "  )\n",
    "SELECT\n",
    "    spending_by_run.workspace_id,\n",
    "    job_metadata.name,\n",
    "    spending_by_run.job_id,\n",
    "    spending_by_run.run_id,\n",
    "    spending_by_run.run_as,\n",
    "    SUM(list_cost) as list_cost,\n",
    "    SUM(effective_cost) as effective_cost,\n",
    "    spending_by_run.custom_tags,\n",
    "    spending_by_run.last_seen_date\n",
    "FROM list_cost_per_job_run spending_by_run\n",
    "  LEFT JOIN most_recent_jobs job_metadata USING (workspace_id, job_id)\n",
    "GROUP BY ALL\n",
    "ORDER BY list_cost DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5c87792-a20c-4a6b-8090-fb59c005f5d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "-- Job spend trends over the past 14 days\n",
    "CREATE OR REPLACE MATERIALIZED VIEW job_spend_trend AS\n",
    "with job_run_timeline_with_cost as (\n",
    "  SELECT\n",
    "    usage_data.*,\n",
    "    usage_data.usage_metadata.job_id as job_id,\n",
    "    usage_data.identity_metadata.run_as as run_as,\n",
    "    usage_data.usage_quantity * pricing_data.pricing.default AS list_cost,\n",
    "    usage_data.usage_quantity * pricing_data.pricing.effective_list.default AS effective_cost,\n",
    "    usage_data.custom_tags\n",
    "  FROM system.billing.usage usage_data\n",
    "    INNER JOIN system.billing.list_prices pricing_data\n",
    "      ON\n",
    "        usage_data.cloud = pricing_data.cloud AND\n",
    "        usage_data.sku_name = pricing_data.sku_name AND\n",
    "        usage_data.usage_start_time >= pricing_data.price_start_time AND\n",
    "        (usage_data.usage_end_time <= pricing_data.price_end_time or pricing_data.price_end_time is NULL)\n",
    "  WHERE\n",
    "    usage_data.billing_origin_product = 'JOBS' AND\n",
    "    usage_data.usage_date >= CURRENT_DATE() - INTERVAL 14 DAY\n",
    "),\n",
    "most_recent_jobs as (\n",
    "  SELECT\n",
    "    *,\n",
    "    ROW_NUMBER() OVER(PARTITION BY workspace_id, job_id ORDER BY change_time DESC) as rn\n",
    "  FROM\n",
    "    system.lakeflow.jobs QUALIFY rn=1\n",
    "),\n",
    "job_spending_aggregated as (\n",
    "  SELECT\n",
    "    workspace_id,\n",
    "    job_id,\n",
    "    run_as,\n",
    "    sku_name,\n",
    "    SUM(list_cost) AS spend,\n",
    "    SUM(CASE WHEN usage_end_time BETWEEN date_add(current_date(), -8) AND date_add(current_date(), -1) THEN effective_cost ELSE 0 END) AS Last7DaySpend,\n",
    "    SUM(CASE WHEN usage_end_time BETWEEN date_add(current_date(), -15) AND date_add(current_date(), -8) THEN effective_cost ELSE 0 END) AS Last14DaySpend,\n",
    "    custom_tags\n",
    "  FROM job_run_timeline_with_cost\n",
    "  GROUP BY ALL\n",
    ")\n",
    "SELECT\n",
    "  job_metadata.name,\n",
    "  spending_by_job.workspace_id,\n",
    "  spending_by_job.job_id,\n",
    "  spending_by_job.sku_name,\n",
    "  spending_by_job.run_as,\n",
    "  spending_by_job.custom_tags,\n",
    "  spending_by_job.Last7DaySpend,\n",
    "  spending_by_job.Last14DaySpend,\n",
    "  spending_by_job.Last7DaySpend - spending_by_job.Last14DaySpend as Last7DayGrowth,\n",
    "  try_divide((spending_by_job.Last7DaySpend - spending_by_job.Last14DaySpend), spending_by_job.Last14DaySpend) * 100 AS Last7DayGrowthPct\n",
    "FROM job_spending_aggregated spending_by_job\n",
    "  LEFT JOIN most_recent_jobs job_metadata USING (workspace_id, job_id)\n",
    "ORDER BY\n",
    "  Last7DayGrowth DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2888705-3a9d-422c-9e8f-8f113d68991b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "-- Analysis of failed jobs and their cost in last 30 days\n",
    "CREATE OR REPLACE MATERIALIZED VIEW failed_jobs_analysis AS\n",
    "with job_run_timeline_with_cost as (\n",
    "  SELECT\n",
    "    usage_data.*,\n",
    "    usage_data.identity_metadata.run_as as run_as,\n",
    "    timeline.job_id,\n",
    "    timeline.run_id,\n",
    "    timeline.result_state,\n",
    "    usage_data.usage_quantity * pricing_data.pricing.default as list_cost,\n",
    "    usage_data.usage_quantity * pricing_data.pricing.effective_list.default as effective_cost,\n",
    "    usage_data.custom_tags\n",
    "  FROM system.billing.usage usage_data\n",
    "    INNER JOIN system.lakeflow.job_run_timeline timeline\n",
    "      ON\n",
    "        usage_data.workspace_id = timeline.workspace_id\n",
    "        AND usage_data.usage_metadata.job_id = timeline.job_id\n",
    "        AND usage_data.usage_metadata.job_run_id = timeline.run_id\n",
    "        AND usage_data.usage_start_time >= date_trunc(\"Hour\", timeline.period_start_time)\n",
    "        AND usage_data.usage_start_time < date_trunc(\"Hour\", timeline.period_end_time) + INTERVAL 1 HOUR\n",
    "    INNER JOIN system.billing.list_prices pricing_data on\n",
    "      usage_data.cloud = pricing_data.cloud and\n",
    "      usage_data.sku_name = pricing_data.sku_name and\n",
    "      usage_data.usage_start_time >= pricing_data.price_start_time and\n",
    "      (usage_data.usage_end_time <= pricing_data.price_end_time or pricing_data.price_end_time is null)\n",
    "  WHERE\n",
    "    usage_data.billing_origin_product = 'JOBS' AND\n",
    "    usage_data.usage_date >= CURRENT_DATE() - INTERVAL 30 DAYS\n",
    "),\n",
    "cumulative_run_status_cost as (\n",
    "  SELECT\n",
    "    workspace_id,\n",
    "    job_id,\n",
    "    run_id,\n",
    "    run_as,\n",
    "    result_state,\n",
    "    usage_end_time,\n",
    "    custom_tags,\n",
    "    SUM(list_cost) OVER (ORDER BY workspace_id, job_id, run_id, usage_end_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_list_cost,\n",
    "    SUM(effective_cost) OVER (ORDER BY workspace_id, job_id, run_id, usage_end_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_effective_cost\n",
    "  FROM job_run_timeline_with_cost\n",
    "  ORDER BY workspace_id, job_id, run_id, usage_end_time\n",
    "),\n",
    "cost_per_status as (\n",
    "  SELECT\n",
    "      workspace_id,\n",
    "      job_id,\n",
    "      run_id,\n",
    "      run_as,\n",
    "      result_state,\n",
    "      usage_end_time,\n",
    "      custom_tags,\n",
    "      cumulative_list_cost - COALESCE(LAG(cumulative_list_cost) OVER (ORDER BY workspace_id, job_id, run_id, usage_end_time), 0) AS result_state_list_cost,\n",
    "      cumulative_effective_cost - COALESCE(LAG(cumulative_effective_cost) OVER (ORDER BY workspace_id, job_id, run_id, usage_end_time), 0) AS result_state_effective_cost\n",
    "  FROM cumulative_run_status_cost\n",
    "  WHERE result_state IS NOT NULL\n",
    "  ORDER BY workspace_id, job_id, run_id, usage_end_time\n",
    "),\n",
    "cost_per_status_agg as (\n",
    "  SELECT\n",
    "    workspace_id,\n",
    "    job_id,\n",
    "    FIRST(run_as, TRUE) as run_as,\n",
    "    FIRST(custom_tags, TRUE) as custom_tags,\n",
    "    SUM(result_state_list_cost) as list_cost,\n",
    "    SUM(result_state_effective_cost) as effective_cost\n",
    "  FROM cost_per_status\n",
    "  WHERE\n",
    "    result_state IN ('ERROR', 'FAILED', 'TIMED_OUT')\n",
    "  GROUP BY ALL\n",
    "),\n",
    "terminal_statuses as (\n",
    "  SELECT\n",
    "    workspace_id,\n",
    "    job_id,\n",
    "    CASE WHEN result_state IN ('ERROR', 'FAILED', 'TIMED_OUT') THEN 1 ELSE 0 END as is_failure,\n",
    "    period_end_time as last_seen_date\n",
    "  FROM system.lakeflow.job_run_timeline\n",
    "  WHERE\n",
    "    result_state IS NOT NULL AND\n",
    "    period_end_time >= CURRENT_DATE() - INTERVAL 30 DAYS\n",
    "),\n",
    "most_recent_jobs as (\n",
    "  SELECT\n",
    "    *,\n",
    "    ROW_NUMBER() OVER(PARTITION BY workspace_id, job_id ORDER BY change_time DESC) as rn\n",
    "  FROM\n",
    "    system.lakeflow.jobs QUALIFY rn=1\n",
    ")\n",
    "SELECT\n",
    "  first(job_metadata.name) as name,\n",
    "  run_status.workspace_id,\n",
    "  run_status.job_id,\n",
    "  COUNT(*) as runs,\n",
    "  failure_costs.run_as,\n",
    "  failure_costs.custom_tags,\n",
    "  SUM(is_failure) as failures,\n",
    "  (1 - COALESCE(try_divide(SUM(is_failure), COUNT(*)), 0)) * 100 as success_ratio,\n",
    "  first(failure_costs.list_cost) as failure_list_cost,\n",
    "  first(failure_costs.effective_cost) as failure_effective_cost,\n",
    "  MAX(run_status.last_seen_date) as last_seen_date\n",
    "FROM terminal_statuses run_status\n",
    "  LEFT JOIN most_recent_jobs job_metadata USING (workspace_id, job_id)\n",
    "  LEFT JOIN cost_per_status_agg failure_costs USING (workspace_id, job_id)\n",
    "GROUP BY ALL\n",
    "ORDER BY failures DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edb07d31-5c0c-4473-9671-d67fc5ac45ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "-- Job retry patterns and cost in the past 30 days\n",
    "CREATE OR REPLACE MATERIALIZED VIEW job_retry_patterns AS\n",
    "with job_run_timeline_with_cost as (\n",
    "  SELECT\n",
    "    usage_data.*,\n",
    "    timeline.job_id,\n",
    "    timeline.run_id,\n",
    "    usage_data.identity_metadata.run_as as run_as,\n",
    "    timeline.result_state,\n",
    "    usage_data.usage_quantity * pricing_data.pricing.default as list_cost,\n",
    "    usage_data.usage_quantity * pricing_data.pricing.effective_list.default as effective_cost,\n",
    "    usage_data.custom_tags\n",
    "  FROM system.billing.usage usage_data\n",
    "    INNER JOIN system.lakeflow.job_run_timeline timeline\n",
    "      ON\n",
    "        usage_data.workspace_id = timeline.workspace_id\n",
    "        AND usage_data.usage_metadata.job_id = timeline.job_id\n",
    "        AND usage_data.usage_metadata.job_run_id = timeline.run_id\n",
    "        AND usage_data.usage_start_time >= date_trunc(\"Hour\", timeline.period_start_time)\n",
    "        AND usage_data.usage_start_time < date_trunc(\"Hour\", timeline.period_end_time) + INTERVAL 1 HOUR\n",
    "    INNER JOIN system.billing.list_prices pricing_data on\n",
    "      usage_data.cloud = pricing_data.cloud and\n",
    "      usage_data.sku_name = pricing_data.sku_name and\n",
    "      usage_data.usage_start_time >= pricing_data.price_start_time and\n",
    "      (usage_data.usage_end_time <= pricing_data.price_end_time or pricing_data.price_end_time is null)\n",
    "  WHERE\n",
    "    usage_data.billing_origin_product = 'JOBS' AND\n",
    "    usage_data.usage_date >= CURRENT_DATE() - INTERVAL 30 DAYS\n",
    "),\n",
    "cumulative_run_status_cost as (\n",
    "  SELECT\n",
    "    workspace_id,\n",
    "    job_id,\n",
    "    run_id,\n",
    "    run_as,\n",
    "    custom_tags,\n",
    "    result_state,\n",
    "    usage_end_time,\n",
    "    SUM(list_cost) OVER (ORDER BY workspace_id, job_id, run_id, usage_end_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_list_cost,\n",
    "    SUM(effective_cost) OVER (ORDER BY workspace_id, job_id, run_id, usage_end_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_effective_cost\n",
    "  FROM job_run_timeline_with_cost\n",
    "  ORDER BY workspace_id, job_id, run_id, usage_end_time\n",
    "),\n",
    "cost_per_status as (\n",
    "  SELECT\n",
    "      workspace_id,\n",
    "      job_id,\n",
    "      run_id,\n",
    "      run_as,\n",
    "      custom_tags,\n",
    "      result_state,\n",
    "      usage_end_time,\n",
    "      cumulative_list_cost - COALESCE(LAG(cumulative_list_cost) OVER (ORDER BY workspace_id, job_id, run_id, usage_end_time), 0) AS result_state_list_cost,\n",
    "      cumulative_effective_cost - COALESCE(LAG(cumulative_effective_cost) OVER (ORDER BY workspace_id, job_id, run_id, usage_end_time), 0) AS result_state_effective_cost\n",
    "  FROM cumulative_run_status_cost\n",
    "  WHERE result_state IS NOT NULL\n",
    "  ORDER BY workspace_id, job_id, run_id, usage_end_time\n",
    "),\n",
    "cost_per_unsuccessful_status_agg as (\n",
    "  SELECT\n",
    "    workspace_id,\n",
    "    job_id,\n",
    "    run_id,\n",
    "    result_state,\n",
    "    first(custom_tags, TRUE) as custom_tags,\n",
    "    first(run_as, TRUE) as run_as,\n",
    "    SUM(result_state_list_cost) as list_cost,\n",
    "    SUM(result_state_effective_cost) as effective_cost\n",
    "  FROM cost_per_status\n",
    "  WHERE\n",
    "    result_state != \"SUCCEEDED\"\n",
    "  GROUP BY ALL\n",
    "),\n",
    "repaired_runs as (\n",
    "  SELECT\n",
    "    workspace_id, job_id, run_id, COUNT(*) as cnt\n",
    "  FROM system.lakeflow.job_run_timeline\n",
    "  WHERE result_state IS NOT NULL\n",
    "  GROUP BY ALL\n",
    "  HAVING cnt > 1\n",
    "),\n",
    "successful_repairs as (\n",
    "  SELECT \n",
    "    timeline.workspace_id, \n",
    "    timeline.job_id, \n",
    "    timeline.run_id, \n",
    "    MAX(timeline.period_end_time) as period_end_time\n",
    "  FROM system.lakeflow.job_run_timeline timeline\n",
    "  JOIN repaired_runs repair_candidates\n",
    "  ON timeline.workspace_id = repair_candidates.workspace_id \n",
    "     AND timeline.job_id = repair_candidates.job_id \n",
    "     AND timeline.run_id = repair_candidates.run_id\n",
    "  WHERE timeline.result_state = \"SUCCEEDED\"\n",
    "  GROUP BY ALL\n",
    "),\n",
    "combined_repairs as (\n",
    "  SELECT\n",
    "    repair_candidates.*,\n",
    "    successful_completion.period_end_time,\n",
    "    repair_candidates.cnt as repairs\n",
    "  FROM repaired_runs repair_candidates\n",
    "    LEFT JOIN successful_repairs successful_completion USING (workspace_id, job_id, run_id)\n",
    "),\n",
    "most_recent_jobs as (\n",
    "  SELECT\n",
    "    *,\n",
    "    ROW_NUMBER() OVER(PARTITION BY workspace_id, job_id ORDER BY change_time DESC) as rn\n",
    "  FROM\n",
    "    system.lakeflow.jobs QUALIFY rn=1\n",
    ")\n",
    "SELECT\n",
    "  last(job_metadata.name) as name,\n",
    "  repair_summary.workspace_id,\n",
    "  repair_summary.job_id,\n",
    "  repair_summary.run_id,\n",
    "  first(failure_costs.run_as, TRUE) as run_as,\n",
    "  first(failure_costs.custom_tags, TRUE) as custom_tags,\n",
    "  first(repair_summary.repairs) - 1 as repairs,\n",
    "  first(failure_costs.list_cost) as repair_list_cost,\n",
    "  first(failure_costs.effective_cost) as repair_effective_cost,\n",
    "  CASE WHEN repair_summary.period_end_time IS NOT NULL \n",
    "       THEN CAST(repair_summary.period_end_time - MIN(timeline_details.period_end_time) as LONG) \n",
    "       ELSE NULL \n",
    "  END AS repair_time_seconds\n",
    "FROM combined_repairs repair_summary\n",
    "  JOIN system.lakeflow.job_run_timeline timeline_details USING (workspace_id, job_id, run_id)\n",
    "  LEFT JOIN most_recent_jobs job_metadata USING (workspace_id, job_id)\n",
    "  LEFT JOIN cost_per_unsuccessful_status_agg failure_costs USING (workspace_id, job_id, run_id)\n",
    "WHERE\n",
    "  timeline_details.result_state IS NOT NULL\n",
    "GROUP BY repair_summary.workspace_id, repair_summary.job_id, repair_summary.run_id, repair_summary.period_end_time\n",
    "ORDER BY repairs DESC;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8034707614283522,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "create_jobs_analytics_mvws",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
